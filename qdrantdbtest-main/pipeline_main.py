import os
import requests
import argparse
from bs4 import BeautifulSoup
from qdrant_client import QdrantClient
from qdrant_client.http import models as qdrant_models
from llama_index.core import Document, VectorStoreIndex
from llama_index.embeddings.huggingface import HuggingFaceEmbedding
from llama_index.core.settings import Settings
from llama_index.vector_stores.qdrant import QdrantVectorStore
from llama_index.core.node_parser import SentenceSplitter
from llama_index.core.storage import StorageContext

def parse_page(url):
    """–ü–∞—Ä—Å–∏–Ω–≥ —Å—Ç—Ä–∞–Ω–∏—Ü—ã —Å —É–ª—É—á—à–µ–Ω–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–æ–π —Ç–µ–∫—Å—Ç–∞"""
    try:
        response = requests.get(url, timeout=15)
        response.raise_for_status()
    except Exception as e:
        print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ {url}: {str(e)}")
        return None

    soup = BeautifulSoup(response.text, 'html.parser')

    # –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ —Å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ–º —Å—Ç—Ä—É–∫—Ç—É—Ä—ã
    text_parts = []
    for tag in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'li', 'pre', 'article']:
        for element in soup.find_all(tag):
            text = element.get_text(" ", strip=True)
            if text and len(text) > 20:
                text_parts.append(text)

    if not text_parts:
        print(f"‚ö† –ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ç–µ–∫—Å—Ç –∏–∑ {url}")
        return None

    full_text = "\n".join(text_parts)
    images = [img['src'] for img in soup.find_all('img', src=True) if img.get('src')]

    return Document(
        text=full_text,
        metadata={
            "url": url,
            "images": images,
            "source": "eltex-docs"
        }
    )

def main(args):
    # –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è
    QDRANT_HOST = args.qdrant_host
    QDRANT_PORT = 6333
    COLLECTION_NAME = args.collection_name
    EMBEDDING_MODEL = "sentence-transformers/all-MiniLM-L6-v2"
    URLS_FILE = "urls.txt"

    # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("üîç –ó–∞–≥—Ä—É–∑–∫–∞ –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤...")
    with open(URLS_FILE) as f:
        urls = [line.strip() for line in f if line.strip()]

    documents = [doc for url in urls if (doc := parse_page(url)) is not None]

    if not documents:
        print("üõë –ù–µ—Ç –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏")
        return

    # 2. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –º–æ–¥–µ–ª–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤
    print("\nüß† –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–æ–¥–µ–ª–∏...")
    Settings.embed_model = HuggingFaceEmbedding(
        model_name=EMBEDDING_MODEL,
        device="cpu"
    )

    # 3. –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant
    print("\nüîå –ü–æ–¥–∫–ª—é—á–µ–Ω–∏–µ –∫ Qdrant...")
    client = QdrantClient(
        host=QDRANT_HOST,
        port=QDRANT_PORT,
        timeout=30,
        prefer_grpc=False  # –í–∞–∂–Ω–æ –æ—Ç–∫–ª—é—á–∏—Ç—å –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏
    )

    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Ç–∞—Ä–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (–¥–ª—è —Ç–µ—Å—Ç–∞)
    try:
        client.delete_collection(COLLECTION_NAME)
        print(f"‚ôª –£–¥–∞–ª–µ–Ω–∞ —Å—Ç–∞—Ä–∞—è –∫–æ–ª–ª–µ–∫—Ü–∏—è {COLLECTION_NAME}")
    except Exception as e:
        print(f"‚ÑπÔ∏è –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –∫–æ–ª–ª–µ–∫—Ü–∏—é: {str(e)}")

    # –°–æ–∑–¥–∞–Ω–∏–µ –Ω–æ–≤–æ–π –∫–æ–ª–ª–µ–∫—Ü–∏–∏ (—Ç–æ–ª—å–∫–æ dense-–≤–µ–∫—Ç–æ—Ä—ã)
    client.create_collection(
        collection_name=COLLECTION_NAME,
        vectors_config=qdrant_models.VectorParams(
            size=384,  # –†–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å –¥–ª—è all-MiniLM-L6-v2
            distance=qdrant_models.Distance.COSINE
        )
    )
    print(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ –∫–æ–ª–ª–µ–∫—Ü–∏—è {COLLECTION_NAME}")

    # 4. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ö—Ä–∞–Ω–∏–ª–∏—â–∞ (–±–µ–∑ –≥–∏–±—Ä–∏–¥–Ω–æ–≥–æ –ø–æ–∏—Å–∫–∞)
    vector_store = QdrantVectorStore(
        client=client,
        collection_name=COLLECTION_NAME,
        enable_hybrid=False  # –ö–ª—é—á–µ–≤–æ–µ –∏—Å–ø—Ä–∞–≤–ª–µ–Ω–∏–µ!
    )

    # 5. –ù–∞—Å—Ç—Ä–æ–π–∫–∞ —Ä–∞–∑–±–∏–µ–Ω–∏—è –Ω–∞ —á–∞–Ω–∫–∏
    node_parser = SentenceSplitter(
        chunk_size=2048,
        chunk_overlap=50
    )

    # 6. –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤
    print("\nüì¶ –ù–∞—á–∞–ª–æ –∏–Ω–¥–µ–∫—Å–∞—Ü–∏–∏...")
    storage_context = StorageContext.from_defaults(vector_store=vector_store)

    index = VectorStoreIndex.from_documents(
        documents=documents,
        storage_context=storage_context,
        transformations=[node_parser],
        metadata_mode="exclude",
        show_progress=True
    )

    # 7. –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
    print("\nüîç –ü—Ä–æ–≤–µ—Ä–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤:")
    collection_info = client.get_collection(COLLECTION_NAME)
    print(f"–í—Å–µ–≥–æ —Ç–æ—á–µ–∫: {collection_info.points_count}")

    if collection_info.points_count == 0:
        print("\n‚ùå –í–µ–∫—Ç–æ—Ä—ã –Ω–µ –¥–æ–±–∞–≤–ª–µ–Ω—ã! –ü—Ä–æ–≤–µ—Ä—å—Ç–µ:")
        print("1. –õ–æ–≥–∏ Qdrant –Ω–∞ —Å–µ—Ä–≤–µ—Ä–µ")
        print("2. –î–æ—Å—Ç—É–ø–Ω–æ—Å—Ç—å –ø–æ—Ä—Ç–∞ 6333")
        print("3. –†–∞–∑–º–µ—Ä —á–∞–Ω–∫–æ–≤ (–ø–æ–ø—Ä–æ–±—É–π—Ç–µ —É–º–µ–Ω—å—à–∏—Ç—å –¥–æ 256)")
    else:
        print("\nüéâ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è —É—Å–ø–µ—à–Ω–æ –∑–∞–≤–µ—Ä—à–µ–Ω–∞!")
        print(f"–î–æ–±–∞–≤–ª–µ–Ω–æ –≤–µ–∫—Ç–æ—Ä–æ–≤: {collection_info.points_count}")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="–ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –¥–æ–∫—É–º–µ–Ω—Ç–æ–≤ –≤ Qdrant.")
    parser.add_argument('--collection-name', type=str, default='test-collection',
                        help='–ù–∞–∑–≤–∞–Ω–∏–µ –∫–æ–ª–ª–µ–∫—Ü–∏–∏ –≤ Qdrant')
    parser.add_argument('--qdrant-host', type=str, default='127.0.0.1',
                        help='IP-–∞–¥—Ä–µ—Å –∏–ª–∏ —Ö–æ—Å—Ç Qdrant —Å–µ—Ä–≤–µ—Ä–∞')

    args = parser.parse_args()

    main(args)
